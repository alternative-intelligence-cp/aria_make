Architectural Specification and Implementation Report: Parallel Execution Engine for AriaBuild
1. Executive Summary and Strategic Context
The evolution of the Aria programming language ecosystem has necessitated a fundamental re-evaluation of its supporting infrastructure. As the complexity of Aria projects scales—from simple scripts to monolithic repositories containing thousands of source modules—the limitations of legacy build automation tools have become increasingly acute. The primary bottleneck is no longer raw compilation throughput but the efficient orchestration of the build process itself. The aria_make project was initiated to address this specific deficit, aiming to deliver a "Build System as Code" paradigm that prioritizes determinism, developer ergonomics, and, crucially, high-performance parallel execution.1
This report provides a comprehensive architectural definition and implementation guide for the "Engine Room" of aria_make: the subsystems responsible for the concurrent scheduling and execution of build targets. While previous phases of the project established the lexical analysis and filesystem discovery layers, the core logic required to transform a static Dependency Graph into a dynamic execution plan remains unimplemented. This gap represents a critical path failure; without a robust scheduler, the build tool is functionally inert, capable only of parsing configuration without producing artifacts.1
The scope of this analysis encompasses the complete design and implementation of three interlocking components: the ThreadPool, the BuildScheduler, and the incremental build logic within the DependencyGraph. The design leverages C++17 standards to ensure portability and performance, utilizing modern concurrency primitives such as std::thread, std::mutex, std::condition_variable, and std::atomic to manage the inherent complexity of parallel processing.
At the heart of the proposed solution is a dynamic implementation of Kahn’s Algorithm for topological sorting. Unlike static build tools that linearize the execution plan prior to build start, the aria_make scheduler operates on a live graph reduction model. This approach allows the system to discover and exploit parallelism in real-time, adapting to the varying execution times of different build steps (e.g., compiling vs. linking) to maximize hardware utilization. Furthermore, the report details a rigorous strategy for incremental builds utilizing std::filesystem::last_write_time, ensuring that the system adheres to the principle of "build only what is necessary".2
The following sections will deconstruct the theoretical underpinnings of Directed Acyclic Graph (DAG) scheduling, explore the architectural trade-offs of various concurrency models, and present a production-ready C++17 implementation that satisfies the stringent performance and reliability requirements of the Aria ecosystem.
2. Architectural Philosophy and System Design
The transition from imperative, whitespace-sensitive build scripts (typified by GNU Make) to the declarative, structured configuration of AriaBuild (ABC format) is not merely a syntactic shift; it represents a fundamental change in how the build process is modeled and executed. In an imperative model, the user describes how to build; in a declarative model, the user describes what the dependencies are, leaving the how—the scheduling and execution strategy—to the tool itself. This shift places a heavy burden on the internal architecture of the build engine to make optimal decisions.
2.1 The "Build as Code" Paradigm and Determinism
The architectural vision for aria_make is grounded in the concept of hermeticity and determinism. A build system must be a function $f(S, E) \rightarrow A$, where $S$ is the source code, $E$ is the environment configuration, and $A$ is the resulting set of artifacts. For any given $S$ and $E$, $A$ must be bit-for-bit identical, regardless of the order in which parallel threads complete their tasks. This requirement for determinism profoundly influences the design of the Scheduler.
In a parallel environment, the order of task completion is non-deterministic due to OS scheduling variance, I/O latency, and thermal throttling of CPU cores. If the build logic relies on side effects or implicit ordering (e.g., "target A usually finishes before target B"), the build becomes flaky. Therefore, the BuildScheduler must enforce strict causal ordering derived solely from the explicit Dependency Graph. Concurrency must be constrained such that two tasks are executed in parallel if and only if there is no path between them in the DAG.
2.2 Concurrency Architecture: The Case for Thread Pools
A critical architectural decision involves the selection of the concurrency model. In the context of a build system, where the unit of work (compiling a file) is relatively heavyweight (milliseconds to seconds) and resource-intensive (memory and I/O), the "One Thread Per Task" model is catastrophic.
The Thundering Herd Problem:
Consider a project with 1,000 independent source files (a "flat" dependency graph). A naive approach that spawns a std::thread for each node in the "ready" set would result in 1,000 simultaneous threads competing for CPU time. This leads to:
1. Context Switching Storms: The operating system kernel spends a disproportionate amount of cycles switching between thread contexts rather than executing user-space compiler code.4
2. Memory Exhaustion: Each thread requires its own stack (typically 1-8 MB), potentially leading to gigabytes of committed memory just for thread management.
3. I/O Thrashing: Concurrent access to the filesystem by hundreds of processes creates contention at the disk controller level, degrading throughput significantly.
The Solution: Fixed-Size Thread Pool:
To mitigate these issues, aria_make adopts a Fixed-Size Thread Pool architecture.4 The number of worker threads is clamped to the hardware concurrency of the host machine (typically queried via std::thread::hardware_concurrency()). This ensures that the system achieves maximum CPU saturation without oversubscription. The Scheduler acts as a gatekeeper, releasing tasks to the pool only as resources become available. This model aligns with the best practices identified in high-performance computing research, where maintaining a 1:1 ratio between software threads and hardware execution units minimizes latency and cache pollution.4
2.3 The Feedback Loop Model
The interaction between the Scheduler and the Thread Pool defines the system's runtime behavior. We employ a "Feedback Loop" model rather than a "Fire and Forget" model.
* Fire and Forget: The scheduler submits tasks and checks status later. This introduces latency; a thread might sit idle after finishing a task before the scheduler notices.
* Feedback Loop: When a worker thread completes a task, it actively notifies the Scheduler (via condition variables or atomic callbacks). The Scheduler immediately performs graph reduction (updating dependency counts) and potentially submits new tasks to the pool before the worker even returns to the idle state. This keeps the execution pipeline full and minimizes the "tail latency" of the build.
3. Theoretical Framework: Directed Acyclic Graph (DAG) Scheduling
Before detailing the C++ implementation, it is essential to establish the rigorous mathematical framework that governs the correctness of the build system. The Dependency Graph is a Directed Acyclic Graph $G = (V, E)$, where $V$ is the set of build targets and $E$ is the set of directed edges representing dependencies. An edge $(u, v)$ indicates that target $u$ depends on target $v$ (or conversely, $v$ must be built before $u$, depending on the edge direction semantics chosen).
3.1 Topological Sorting
The fundamental operation required to execute a build is Topological Sorting. This is a linear ordering of vertices such that for every directed edge from $u$ to $v$, vertex $u$ comes before $v$ in the ordering.6 This linear order represents a valid sequential build plan.
However, aria_make is a parallel build system. A single linear order is insufficient because it artificially serializes independent tasks. For example, if $A$ depends on $B$ and $C$, a topological sort might yield


$$or$$
. Both are valid, but both imply a sequence ($B$ then $C$, or $C$ then $B$). In reality, $B$ and $C$ are independent and should be executed simultaneously.
3.2 Kahn’s Algorithm: The Dynamic Approach
To solve the parallelism problem, aria_make utilizes a dynamic variation of Kahn’s Algorithm.7 Kahn's algorithm is naturally suited for parallel scheduling because it relies on the concept of "In-Degree"—the count of incoming edges to a node.
Algorithm Mechanics:
1. Initialization: Compute the in-degree of every node in the graph. In a build graph where edges point from Dependent $\to$ Dependency ($A \to B$ means A needs B), the in-degree represents the number of prerequisites a target has.
   * Correction: In standard build logic, we usually model edges as Dependency $\to$ Dependent ($B \to A$ means B allows A). In this model, the in-degree of $A$ is the number of unsatisfied dependencies. When $B$ finishes, we traverse the edge $B \to A$ and decrement $A$'s in-degree.
2. Frontier Detection: Identify the set $S$ of all nodes with in-degree 0. These nodes have no unsatisfied dependencies and are ready to build immediately.
3. Parallel Dispatch: Submit all nodes in $S$ to the Thread Pool.
4. Graph Reduction (The Loop):
   * Wait for a node $u$ to complete execution.
   * For each neighbor $v$ of $u$ (where $u \to v$ is an edge):
      * Decrement in-degree of $v$.
      * If in-degree of $v$ becomes 0, add $v$ to the Thread Pool.
5. Termination: Repeat until all nodes are processed.
This algorithm effectively "peels" the graph layer by layer, exposing the maximum amount of available parallelism at every step without requiring complex lookahead or heuristics.7
3.3 Cycle Detection Theory
A critical requirement for the scheduler is robust cycle detection.1 A cycle in a dependency graph ($A \to B \to A$) represents a logical paradox: $A$ cannot start until $B$ finishes, and $B$ cannot start until $A$ finishes. This leads to a deadlock where the build hangs indefinitely.
While Kahn's algorithm implicitly detects cycles (if the queue empties but nodes remain in the graph with non-zero in-degrees, a cycle exists), it does not inherently produce the path of the cycle, which is required for user-friendly error reporting ("Error: Cycle detected A -> B -> A").
To satisfy the reporting requirement, the system must employ a secondary algorithm when Kahn's algorithm fails: Depth-First Search (DFS) with a recursion stack tracker.
* White Set: Unvisited nodes.
* Gray Set: Nodes currently in the recursion stack (visiting).
* Black Set: Fully visited nodes.
A cycle is detected if the DFS encounters a node that is currently in the Gray Set.9 This indicates a back-edge to an active ancestor. The recursion stack at that moment represents the exact path of the cycle.
4. Concurrency Architecture: The Thread Pool Implementation
The ThreadPool component is the engine of the build system. Its implementation must be robust, efficient, and free of race conditions. We adhere to C++17 best practices, utilizing std::vector<std::thread> for worker management and std::condition_variable for task signaling.5
4.1 Implementation Considerations
The gap analysis identified that the ThreadPool was missing from the provided sources.1 The following implementation fills this gap. It is designed to be a generic task executor, accepting std::function<void()> tasks. This abstraction allows the Scheduler to submit complex operations (e.g., "Build Node X, then update Graph, then notify Scheduler") wrapped in lambdas, decoupling the Thread Pool from the specific logic of the build graph.
Key Design Decisions:
* Mutex Granularity: A single mutex protects the task queue. While lock-free queues exist, the overhead of a mutex is negligible compared to the duration of a compiler invocation (milliseconds vs. microseconds). Correctness and maintainability are prioritized over micro-optimizations.
* Shutdown Protocol: The destructor must ensure all threads are joined. We employ a stop_flag boolean. When set, workers act as "drainers," finishing the remaining queue but accepting no new tasks, before exiting.
4.2 C++17 ThreadPool Source Code


C++




/**
* @file thread_pool.h
* @brief Fixed-size thread pool implementation for AriaBuild.
*
* This component manages a pool of worker threads to execute build tasks concurrently.
* It implements a producer-consumer pattern with graceful shutdown capabilities.
*/

#pragma once

#include <vector>
#include <queue>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <functional>
#include <future>
#include <atomic>
#include <memory>
#include <type_traits>

namespace aria {
namespace runtime {

class ThreadPool {
public:
   /**
    * @brief Constructs the thread pool.
    * @param num_threads Number of worker threads to spawn. 
    *                    Defaults to std::thread::hardware_concurrency().
    */
   explicit ThreadPool(size_t num_threads = std::thread::hardware_concurrency()) 
       : stop_flag_(false) 
   {
       // Safety check: Ensure at least one thread exists even if hardware detection fails
       size_t threads = (num_threads > 0)? num_threads : 1;
       
       workers_.reserve(threads);
       for (size_t i = 0; i < threads; ++i) {
           workers_.emplace_back([this] {
               this->worker_loop();
           });
       }
   }

   /**
    * @brief Destructor. Signals stop and joins all threads.
    */
   ~ThreadPool() {
       shutdown();
   }

   // Disable copy and move to ensure stable thread management
   ThreadPool(const ThreadPool&) = delete;
   ThreadPool& operator=(const ThreadPool&) = delete;

   /**
    * @brief Enqueues a task for execution.
    * @param task The callable object (function, lambda) to execute.
    */
   template<class F>
   void enqueue(F&& task) {
       {
           std::unique_lock<std::mutex> lock(queue_mutex_);
           if (stop_flag_) {
               throw std::runtime_error("enqueue on stopped ThreadPool");
           }
           // Use std::forward to preserve value category
           tasks_.emplace(std::forward<F>(task));
       }
       // Notify one worker to wake up
       condition_.notify_one();
   }

private:
   std::vector<std::thread> workers_;
   std::queue<std::function<void()>> tasks_;
   
   std::mutex queue_mutex_;
   std::condition_variable condition_;
   bool stop_flag_;

   /**
    * @brief Main loop executed by worker threads.
    */
   void worker_loop() {
       while (true) {
           std::function<void()> task;

           {
               std::unique_lock<std::mutex> lock(queue_mutex_);
               
               // Wait until task is available OR pool is stopped
               condition_.wait(lock, [this] {
                   return this->stop_flag_ ||!this->tasks_.empty();
               });

               // Exit condition: Stop flag is set AND queue is empty.
               // This ensures we drain the queue before shutting down.
               if (this->stop_flag_ && this->tasks_.empty()) {
                   return;
               }

               // Move task from queue to local storage
               task = std::move(this->tasks_.front());
               this->tasks_.pop();
           }

           // Execute task outside the lock to allow other threads to access the queue
           try {
               task();
           } catch (...) {
               // In a robust system, exceptions should be captured and reported.
               // For aria_make, the task wrapper in the Scheduler handles specific logic errors.
               // This catch prevents a rogue exception from terminating the worker thread.
           }
       }
   }

   void shutdown() {
       {
           std::unique_lock<std::mutex> lock(queue_mutex_);
           stop_flag_ = true;
       }
       // Wake up all threads so they can see stop_flag_
       condition_.notify_all();
       
       for (std::thread &worker : workers_) {
           if (worker.joinable()) {
               worker.join();
           }
       }
   }
};

} // namespace runtime
} // namespace aria

4.3 Implementation Analysis
Memory Model & Ordering:
The implementation relies on the sequential consistency guarantees of std::mutex. When a thread releases the mutex after pushing a task, and another thread acquires it to pop the task, a "happens-before" relationship is established. This ensures that any memory writes performed by the producer (e.g., setting up the Node state) are visible to the worker thread before it executes the task. We avoid relaxed atomic operations here because the complexity outweighs the benefit for a coarse-grained task queue.
Exception Safety:
The enqueue method throws if called after shutdown, protecting the invariant of the pool. The worker_loop includes a try-catch block around task(). This is crucial; if a user-submitted lambda throws an exception, it would otherwise propagate up and call std::terminate, crashing the entire build tool. By catching it (even if just to swallow it or log it), we ensure the worker thread survives to process the next task.
5. Data Structure Design: The Dependency Graph
The DependencyGraph is the shared state accessed by the concurrent scheduler. The gap analysis revealed that while the basic structure was conceptualized, the critical components for thread safety and cycle detection were broken or missing.1
5.1 The Node Class: Atomic State Management
The Node class must handle concurrent updates. Specifically, the in_degree counter is a hotspot. In a "Diamond Dependency" scenario (A depends on B and C), both B and C might finish simultaneously on different threads and attempt to decrement A's in-degree.
Mutex vs. Atomics:
We could protect in_degree with a std::mutex. However, acquiring a mutex involves a system call (if contended) or complex compare-and-swap loops. Since the operation is a simple integer decrement, std::atomic<int> is the superior choice. It maps directly to hardware instructions (e.g., LOCK DEC on x86), providing significantly higher throughput with zero risk of deadlock at the node level.10
5.2 Cycle Detection Implementation
The CycleDetector implements the DFS logic described in Section 3.3. The snippet provided in the research materials 1 was incomplete. The version below includes the critical "backtracking" step (recursion_stack_[u] = false) which distinguishes a cycle from a shared dependency. It also implements the reconstruct_path method to provide the actionable error message required by the spec ("A -> B -> C -> A").
5.3 C++17 Dependency Graph Implementation


C++




/**
* @file dependency_graph.h
* @brief Thread-safe graph data structures and cycle detection logic.
*/

#pragma once

#include <string>
#include <vector>
#include <atomic>
#include <mutex>
#include <memory>
#include <unordered_map>
#include <filesystem>
#include <iostream>
#include <algorithm>

namespace aria {
namespace graph {

class Node {
public:
   enum class Status {
       NotStarted,
       Pending,    // In queue
       Building,   // Executing
       Completed,  // Done success
       Skipped,    // Up-to-date
       Failed
   };

   explicit Node(std::string name)
       : name_(std::move(name)), in_degree_(0), status_(Status::NotStarted) {}

   // Delete copy/move to maintain pointer stability in the graph
   Node(const Node&) = delete;
   Node& operator=(const Node&) = delete;

   // --- Graph Topology ---
   void add_dependent(Node* node) { dependents_.push_back(node); }
   void add_dependency(Node* node) { dependencies_.push_back(node); }

   const std::vector<Node*>& dependents() const { return dependents_; }
   const std::vector<Node*>& dependencies() const { return dependencies_; }
   const std::string& name() const { return name_; }

   // --- Atomic State for Scheduler ---
   int get_in_degree() const { return in_degree_.load(); }
   void increment_in_degree() { in_degree_++; }
   
   // Returns the value AFTER decrement. This is critical for the "Is it zero?" check.
   int decrement_in_degree() { return --in_degree_; } 

   // --- Build Configuration ---
   std::string command;
   std::string output_file;
   std::vector<std::string> source_files;
   
   // Status management (not atomic, protected by Scheduler lock usually, 
   // or atomic if accessed purely for reporting)
   void set_status(Status s) { status_ = s; }
   Status get_status() const { return status_.load(); }

private:
   std::string name_;
   std::vector<Node*> dependents_;   // Outgoing edges (This -> Others)
   std::vector<Node*> dependencies_; // Incoming edges (Others -> This)
   
   std::atomic<int> in_degree_;      // Thread-safe counter
   std::atomic<Status> status_;
};

// --- Cycle Detector Helper ---
class CycleDetector {
public:
   // Returns the cycle path if found, empty vector otherwise
   std::vector<std::string> find_cycle(const std::vector<std::unique_ptr<Node>>& nodes) {
       for (const auto& node_ptr : nodes) {
           if (!visited_[node_ptr.get()]) {
               if (dfs(node_ptr.get())) {
                   return reconstruct_path();
               }
           }
       }
       return {};
   }

private:
   std::unordered_map<Node*, bool> visited_;
   std::unordered_map<Node*, bool> recursion_stack_; // Gray set
   std::vector<Node*> path_stack_;

   bool dfs(Node* u) {
       visited_[u] = true;
       recursion_stack_[u] = true;
       path_stack_.push_back(u);

       for (Node* v : u->dependents()) {
           if (!visited_[v]) {
               if (dfs(v)) return true;
           } else if (recursion_stack_[v]) {
               // Cycle Detected: v is already in the active recursion stack
               path_stack_.push_back(v); // Close the loop visually
               return true;
           }
       }

       // Backtracking: Remove from recursion stack
       recursion_stack_[u] = false;
       path_stack_.pop_back();
       return false;
   }

   std::vector<std::string> reconstruct_path() {
       std::vector<std::string> path;
       // The path_stack contains the full traversal history.
       // We need to slice it from the first occurrence of the repeated node.
       if (path_stack_.empty()) return path;
       
       Node* start_node = path_stack_.back();
       bool recording = false;
       
       for (const auto& node : path_stack_) {
           if (node == start_node) recording = true;
           if (recording) path.push_back(node->name());
       }
       return path;
   }
};

class DependencyGraph {
public:
   Node* get_or_create_node(const std::string& name) {
       if (node_map_.find(name) == node_map_.end()) {
           auto node = std::make_unique<Node>(name);
           node_map_[name] = node.get();
           nodes_.push_back(std::move(node));
       }
       return node_map_[name];
   }

   const std::vector<std::unique_ptr<Node>>& nodes() const { return nodes_; }
   
   std::vector<std::string> check_for_cycles() const {
       CycleDetector detector;
       return detector.find_cycle(nodes_);
   }

private:
   std::vector<std::unique_ptr<Node>> nodes_;
   std::unordered_map<std::string, Node*> node_map_;
};

} // namespace graph
} // namespace aria

6. Implementation: Incremental Build Logic
An essential feature of any modern build tool is the ability to skip work that is already done. This is the domain of the Incremental Build Logic. The requirement is to implement "Dirty Checking" based on file timestamps.
6.1 The Timestamp Challenge in C++17
The C++17 standard introduced std::filesystem::last_write_time. However, as noted in the research, file_time_type is implementation-defined. It might wrap a timespec on Linux or a FILETIME on Windows.2 Crucially, it is not strictly convertible to std::time_t in a cross-platform way in C++17 (though C++20 fixes this with std::chrono::file_clock).
For aria_make, the robust solution is to perform comparisons purely within the file_time_type domain. We do not need to know when a file was modified (e.g., "Dec 19, 2025"), only if it was modified after another file. operator> is well-defined for file_time_type.
6.2 The Recursive Definition of "Dirty"
A node is considered "Dirty" (needs building) if:
1. Output Missing: The target file does not exist.
2. Source Modified: Any source file is newer than the output file.
3. Dependency Dirty: Any dependency node is marked as Dirty.
4. Dependency Newer: The output file of a dependency is newer than the target's output file (implies the dependency was just rebuilt).
Condition 3 is subtle. In a dynamic scheduler, we calculate dirtiness just before scheduling. If a dependency was rebuilt, it will have a brand new timestamp. Therefore, Condition 4 covers Condition 3 implicitly in most cases, but explicit status checks are safer.
6.3 Code Implementation


C++




/**
* @file incremental_logic.cpp
* @brief Filesystem timestamp analysis.
*/

#include "graph/dependency_graph.h"
#include <filesystem>
#include <system_error>

namespace aria {
namespace build {

namespace fs = std::filesystem;

bool check_is_dirty(graph::Node* node) {
   // Case 0: Phony targets (no output file) are always dirty
   // e.g., "clean", "all", or "test" targets.
   if (node->output_file.empty()) {
       return true;
   }

   fs::path out_path(node->output_file);
   std::error_code ec;

   // Case 1: Output file missing
   if (!fs::exists(out_path, ec)) {
       return true; 
   }

   // Get output timestamp. If error (e.g. permission), assume dirty to be safe.
   auto out_time = fs::last_write_time(out_path, ec);
   if (ec) return true;

   // Case 2: Source file newer than output
   for (const auto& src : node->source_files) {
       fs::path src_path(src);
       if (!fs::exists(src_path, ec)) {
           // Missing source is usually a fatal error, but for dirty check logic
           // it definitely means the state is invalid/dirty.
           return true; 
       }

       auto src_time = fs::last_write_time(src_path, ec);
       if (src_time > out_time) {
           return true;
       }
   }

   // Case 3: Dependencies
   for (const auto* dep : node->dependencies()) {
       // If dependency has no output, it's a phony target.
       // If a phony dependency runs, do we rebuild? 
       // Conservative answer: Yes.
       if (dep->output_file.empty()) return true;

       fs::path dep_out_path(dep->output_file);
       if (fs::exists(dep_out_path, ec)) {
           auto dep_time = fs::last_write_time(dep_out_path, ec);
           // If dependency is newer (e.g. it was just rebuilt), we must rebuild.
           if (dep_time > out_time) {
               return true;
           }
       } else {
           // Dependency output missing means it failed or hasn't run.
           // In a valid schedule, it should have run. 
           // If it's missing, we are definitely dirty.
           return true;
       }
   }

   return false; // Up to date
}

} // namespace build
} // namespace aria

7. The Scheduling Engine: BuildScheduler
The BuildScheduler serves as the conductor of the build orchestra. It integrates the DependencyGraph, ThreadPool, and check_is_dirty logic into a coherent execution loop.
7.1 Dynamic Graph Reduction Strategy
The core of the scheduler is the "completion callback." When a task finishes:
1. Lock Scheduler State: Acquire mutex.
2. Update Status: Mark node as Completed/Failed.
3. Propagate: Iterate over the node's dependents (outgoing edges).
4. Decrement: Call dependent->decrement_in_degree().
5. Trigger: If the new in-degree is 0, the dependent is now unlocked. Push it to the ThreadPool.
6. Notify: Signal the condition variable to inform the main thread of progress.
This logic allows the graph to "unroll" dynamically. A complex diamond dependency like $A \to B, A \to C, B \to D, C \to D$ handles itself automatically:
1. $D$ finishes.
2. $B$ and $C$ unlock (in-degree becomes 0).
3. $B$ and $C$ run in parallel.
4. $A$ waits. It has in-degree 2.
5. $B$ finishes. $A$'s in-degree becomes 1.
6. $C$ finishes. $A$'s in-degree becomes 0.
7. $A$ unlocks.
7.2 Process Execution Wrapper
To invoke the compiler, we use popen to capture output. While std::system is simpler, it dumps output to stdout, causing interleaved garbage when multiple threads run. popen allows us to capture the output into a string and print it atomically upon completion.1


C++




#include <cstdio>
#include <array>

struct ExecResult {
   int exit_code;
   std::string output;
};

// G10 Requirement: Process spawning and output capture
ExecResult execute_command(const std::string& cmd) {
   if (cmd.empty()) return {0, ""};
   
   // Merge stderr into stdout to capture compiler warnings/errors
   std::string full_cmd = cmd + " 2>&1"; 
   std::string output;
   std::array<char, 256> buffer;

   // Platform-specific pipe opening
#ifdef _WIN32
   FILE* pipe = _popen(full_cmd.c_str(), "r");
#else
   FILE* pipe = popen(full_cmd.c_str(), "r");
#endif

   if (!pipe) return {-1, "Failed to open pipe"};

   while (fgets(buffer.data(), buffer.size(), pipe)!= nullptr) {
       output += buffer.data();
   }

#ifdef _WIN32
   int rc = _pclose(pipe);
#else
   int rc = pclose(pipe);
   if (WIFEXITED(rc)) rc = WEXITSTATUS(rc);
#endif

   return {rc, output};
}

7.3 BuildScheduler Source Code


C++




/**
* @file build_scheduler.h
* @brief Dynamic Kahn's Algorithm Scheduler.
*/

#pragma once

#include "graph/dependency_graph.h"
#include "runtime/thread_pool.h"
#include <mutex>
#include <condition_variable>
#include <iostream>

namespace aria {
namespace build {

class BuildScheduler {
public:
   BuildScheduler(graph::DependencyGraph& graph, size_t num_threads)
       : graph_(graph), pool_(num_threads), 
         tasks_in_flight_(0), build_failed_(false) {}

   /**
    * @brief Executes the build process.
    * @return true if successful, false otherwise.
    */
   bool execute() {
       // Step 1: Initial Scan (The "Seeds")
       // Find all nodes that are initially ready (in-degree 0)
       {
           std::lock_guard<std::mutex> lock(scheduler_mutex_);
           for (const auto& node_ptr : graph_.nodes()) {
               if (node_ptr->get_in_degree() == 0) {
                   schedule_node_unlocked(node_ptr.get());
               }
           }
       }

       // Step 2: Event Loop
       // Wait until the build is fully complete.
       std::unique_lock<std::mutex> lock(scheduler_mutex_);
       scheduler_cv_.wait(lock, [this] {
           // Stop if:
           // 1. Build already failed (Fail-Fast)
           // 2. No tasks running AND no tasks pending (Success)
           // Note: ready_queue isn't explicitly tracked here because tasks 
           // are immediately pushed to ThreadPool. tasks_in_flight_ covers both
           // "queued in pool" and "executing in pool".
           return build_failed_ |

| tasks_in_flight_ == 0;
       });

       return!build_failed_;
   }

private:
   graph::DependencyGraph& graph_;
   runtime::ThreadPool pool_;
   
   std::mutex scheduler_mutex_;
   std::condition_variable scheduler_cv_;
   
   int tasks_in_flight_;
   bool build_failed_;

   // Must be called with lock held
   void schedule_node_unlocked(graph::Node* node) {
       // Dirty Check
       bool is_dirty = false;
       try {
           is_dirty = check_is_dirty(node);
       } catch (const std::exception& e) {
           std::cerr << "Internal Error: " << e.what() << "\n";
           build_failed_ = true;
           scheduler_cv_.notify_all();
           return;
       }

       if (is_dirty) {
           node->set_status(graph::Node::Status::Pending);
           tasks_in_flight_++;
           
           // Submit Task to Pool
           pool_.enqueue([this, node] {
               this->worker_task(node);
           });
       } else {
           // Node is up to date. Mark skipped.
           node->set_status(graph::Node::Status::Skipped);
           // Even if skipped, we must unlock its dependents!
           // We treat this as an instant "success".
           process_completion_unlocked(node, true);
       }
   }

   // The logic running inside the ThreadPool worker
   void worker_task(graph::Node* node) {
       node->set_status(graph::Node::Status::Building);

       // Execute Command
       // Note: No lock held here! Parallelism happens here.
       ExecResult res = execute_command(node->command);
       
       bool success = (res.exit_code == 0);

       if (success) {
           node->set_status(graph::Node::Status::Completed);
           // Log success (optional, keep quiet for clean output)
       } else {
           node->set_status(graph::Node::Status::Failed);
           // Atomic console output for errors
           static std::mutex io_mutex;
           std::lock_guard<std::mutex> io_lock(io_mutex);
           std::cerr << "FAILED: " << node->name() << "\n";
           std::cerr << res.output << "\n";
       }

       // Report back to Scheduler
       {
           std::lock_guard<std::mutex> lock(scheduler_mutex_);
           tasks_in_flight_--;
           process_completion_unlocked(node, success);
       }
       
       // Wake up main thread to check exit conditions
       scheduler_cv_.notify_all();
   }

   // Handle graph reduction. Must be called with lock held.
   void process_completion_unlocked(graph::Node* node, bool success) {
       if (!success) {
           build_failed_ = true;
           return; // Stop propagating
       }

       // Unlock Dependents
       for (auto* dependent : node->dependents()) {
           // Atomic decrement returns new value
           int new_degree = dependent->decrement_in_degree();
           
           if (new_degree == 0) {
               // Dependency satisfied! Schedule it.
               schedule_node_unlocked(dependent);
           }
       }
   }
};

} // namespace build
} // namespace aria

8. Performance and Scalability Analysis
The architecture detailed above is designed to scale with the complexity of the project and the capability of the hardware.
8.1 Complexity Analysis
* Space Complexity: $O(V + E)$ to store the graph (Nodes + Adjacency lists). The ThreadPool and Scheduler introduce negligible $O(T)$ overhead where $T$ is the thread count.
* Time Complexity (Scheduling): The scheduling logic (Kahn's algorithm) visits every edge exactly once. Decrementing an atomic integer is $O(1)$. Thus, the scheduling overhead is $O(V + E)$.
* Time Complexity (Execution): The total time is governed by the Critical Path of the DAG. The dynamic scheduler guarantees that if there are $N$ tasks ready and $K$ threads available, $\min(N, K)$ tasks will execute.
8.2 Architectural Trade-offs
Atomic vs. Mutex for Node State:
We chose std::atomic<int> for in-degree. This avoids a global lock or per-node mutex during the "unlocking" phase. In a massive build with high connectivity (e.g., a header used by 10,000 files), 10,000 threads might try to decrement the header's dependent count. Atomics handle this contention at the hardware cache coherence level, which is orders of magnitude faster than OS-level mutex arbitration.
Condition Variable for Task Queue:
While spinlocks can be faster for extremely low-latency queues, they waste CPU cycles. In a build system, tasks (compilation) take milliseconds or seconds. The overhead of sleeping a thread via condition_variable (microseconds) is irrelevant, and the power saving is significant.
9. Conclusion
This report has presented a comprehensive specification for the execution engine of aria_make. By filling the identified gaps—specifically the ThreadPool implementation, the CycleDetector repair, and the BuildScheduler logic—we have transformed the abstract design into a concrete, actionable blueprint.
The resulting system is robust:
* Correct: It strictly enforces dependency ordering via topological sort and detects cycles.
* Performant: It utilizes a thread pool to saturate CPU cores without oversubscription.
* Efficient: It employs timestamp-based incremental logic to minimize redundant work.
* Maintainable: It leverages standard C++17 idioms (unique_ptr, atomic, filesystem) ensuring the codebase remains accessible and portable.
This implementation provides the necessary foundation for the Aria language ecosystem to mature, offering developers a build experience that is as modern and capable as the language itself.
Works cited
1. 01_project_overview.txt
2. Filesystem library (since C++17) - cppreference.com - C++ Reference, accessed December 19, 2025, https://en.cppreference.com/w/cpp/filesystem.html
3. std::filesystem::last_write_time - cppreference.com - C++ Reference, accessed December 19, 2025, https://en.cppreference.com/w/cpp/filesystem/last_write_time.html
4. A simple and fast C++ thread pool implementation capable of running task graphs - arXiv, accessed December 19, 2025, https://arxiv.org/html/2407.15805v2
5. Thread Pool in C++ - GeeksforGeeks, accessed December 19, 2025, https://www.geeksforgeeks.org/cpp/thread-pool-in-cpp/
6. Topological sorting - Wikipedia, accessed December 19, 2025, https://en.wikipedia.org/wiki/Topological_sorting
7. Kahn's Algorithm in C++ - GeeksforGeeks, accessed December 19, 2025, https://www.geeksforgeeks.org/cpp/kahns-algorithm-in-cpp/
8. Kahn's Algorithm in C Language - GeeksforGeeks, accessed December 19, 2025, https://www.geeksforgeeks.org/c/kahns-algorithm-in-c-language/
9. Kahn's algorithm vs DFS for course schedule leetcode - Stack Overflow, accessed December 19, 2025, https://stackoverflow.com/questions/68307377/kahns-algorithm-vs-dfs-for-course-schedule-leetcode
10. Managing threads while practicing modern c++17's best practices - Stack Overflow, accessed December 19, 2025, https://stackoverflow.com/questions/49439929/managing-threads-while-practicing-modern-c17s-best-practices